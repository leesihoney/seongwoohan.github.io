<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="shortcut icon" href="../../assets/ico/favicon.png"> -->

    <title>Seong Woo Han</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="main.css" rel="stylesheet">
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=96_cPsU0hobTwkvz5TE65OElBK1GlwPvk51w9wGRAAQ"></script>-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.js"></script>
    <script src="../../assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="container">
      <br>
      <h2>Seong Woo Han</h2>
      <div class="row" style="padding: 0px 15px 0px 15px;">
        <div class="col-sm-8">
          <br>
          I'm a senior math student at <a href="https://cims.nyu.edu//index.html" target="_blank">Courant Institute of Mathematical Sciences, New York University</a>.
          I'm broadly interested in mathematical modeling, probabilistic inference, and machine learning applications in bioscience and healthcare. 
          I am looking to explore understanding fariness and biases in machine learning and methods that improve the accuracy of models on minorities.
          <br>
          </p>
            <p style="text-align:center">
             <a href="mailto:swh324@nyu.edu">Email</a> &nbsp/&nbsp
             <a href="https://github.com/seongwoohan">Github</a> &nbsp/&nbsp
             <a href="https://twitter.com/seongwoohan29">Twitter</a> 
          </p>       
        </div>
        <div class="col-sm-4" style="padding: 15px 15px 15px 15px;">
          <img class="profile" src="imgs/seongwoohan.png"/>
        </div>
      </div>

      <hr>

      <h3>Research</h3>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">

    <tr>
      <td width="25%">
        <img src='Images/presgan.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/PRESGAN.pdf">
            <papertitle>Prescribed Generative Adversarial Networks</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>
    <br>
        <em>Journal of Machine Learning Research (JMLR) (Submitted)</em>  <br>
        <a href="https://arxiv.org/abs/1910.04302">arxiv</a>
        /
        <a href="https://github.com/adjidieng/PresGANs">Code</a>
         / 
        </p>
        <p>This paper describes a solution to two important problems in the GAN literature: (1) How can we maximize the entropy of the generator of a GAN to prevent mode collapse? (2) How can we evaluate predictive log-likelihood for GANs to assess how they generalize to new data? Key ingredients: noise, entropy regularization, and Hamiltonian Monte Carlo. </p>
      </td>
    
    
    <tr>
      <td width="25%">
        <img src='Images/presgan.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/PRESGAN.pdf">
            <papertitle>Prescribed Generative Adversarial Networks</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>
    <br>
        <em>Journal of Machine Learning Research (JMLR) (Submitted)</em>  <br>
        <a href="https://arxiv.org/abs/1910.04302">arxiv</a>
        /
        <a href="https://github.com/adjidieng/PresGANs">Code</a>
         / 
        </p>
        <p>This paper describes a solution to two important problems in the GAN literature: (1) How can we maximize the entropy of the generator of a GAN to prevent mode collapse? (2) How can we evaluate predictive log-likelihood for GANs to assess how they generalize to new data? Key ingredients: noise, entropy regularization, and Hamiltonian Monte Carlo. </p>
      </td>
   
    
    <tr>
      <td width="25%">
        <img src='Images/presgan.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/PRESGAN.pdf">
            <papertitle>Prescribed Generative Adversarial Networks</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>
    <br>
        <em>Journal of Machine Learning Research (JMLR) (Submitted)</em>  <br>
        <a href="https://arxiv.org/abs/1910.04302">arxiv</a>
        /
        <a href="https://github.com/adjidieng/PresGANs">Code</a>
         / 
        </p>
        <p>This paper describes a solution to two important problems in the GAN literature: (1) How can we maximize the entropy of the generator of a GAN to prevent mode collapse? (2) How can we evaluate predictive log-likelihood for GANs to assess how they generalize to new data? Key ingredients: noise, entropy regularization, and Hamiltonian Monte Carlo. </p>
      </td>
        
    <tr>
      <td width="25%">
        <img src='Images/presgan.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/PRESGAN.pdf">
            <papertitle>Prescribed Generative Adversarial Networks</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>
    <br>
        <em>Journal of Machine Learning Research (JMLR) (Submitted)</em>  <br>
        <a href="https://arxiv.org/abs/1910.04302">arxiv</a>
        /
        <a href="https://github.com/adjidieng/PresGANs">Code</a>
         / 
        </p>
        <p>This paper describes a solution to two important problems in the GAN literature: (1) How can we maximize the entropy of the generator of a GAN to prevent mode collapse? (2) How can we evaluate predictive log-likelihood for GANs to assess how they generalize to new data? Key ingredients: noise, entropy regularization, and Hamiltonian Monte Carlo. </p>
      </td>
    
        
      </table>
    
      <footer>
        <p>Last update: Dec, 2019</p>
      </footer>
    </div> <!-- /container -->
  </body>
</html>
